{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8482,
     "status": "ok",
     "timestamp": 1716240422094,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "WFImZyEAyTfh"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18, ResNet18_Weights #pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1716240422094,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "uKTeRx25zo2k"
   },
   "outputs": [],
   "source": [
    "# Define the training and validation transforms for the data, add data aug to increase dateset sample\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Flips images vertically\n",
    "    transforms.RandomRotation(30),  # Increase rotation range. I've tried 10 and 20\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),  # Increase color jitter\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),  # Wider scale for random cropping\n",
    "    transforms.RandomGrayscale(p=0.2),  # Randomly convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1716240422094,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "6luU4AvnzsJg"
   },
   "outputs": [],
   "source": [
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1716240422094,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "gMCSeNXBz4gg"
   },
   "outputs": [],
   "source": [
    "# Load the entire dataset\n",
    "data_dir = '/content/drive/MyDrive/AI Bootcamp/animals/animals'\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1716240422094,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "rezxHu5v2oTv"
   },
   "outputs": [],
   "source": [
    "# Define Animal Classes\n",
    "animal_classes = ['cat', 'cow', 'coyote', 'deer', 'dog', 'donkey', 'fox', 'horse', 'owl', 'pig', 'possum', 'raccoon', 'sheep', 'wolf']\n",
    "\n",
    "# Define predator mapping (0: nonpredator, 1: predator, 2: both)\n",
    "predator_mapping = {'cat': 2, 'cow': 0, 'coyote': 1, 'deer': 2, 'dog': 2, 'donkey': 0, 'fox': 1, 'horse': 0, 'owl': 1, 'pig': 0, 'possum': 1,\n",
    "                    'raccoon': 1, 'sheep': 0, 'wolf': 1}\n",
    "\n",
    "predator_classes = ['nonpredator', 'predator', 'both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1716240422306,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "kj84j5Iv3OYI"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1716240422307,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "GW5DS63F3boL"
   },
   "outputs": [],
   "source": [
    "# Apply validation transforms to val_dataset\n",
    "val_dataset.dataset.transform = val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1716240422308,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "kS38HgQQ3edO"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 64  # Adjust batch size. Tried 32 and 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2) # Changed num_workers from 4 to 2 based on Colab warning\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716240422308,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "sy-7ah163vtA"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "class AnimalClassifier(nn.Module):\n",
    "    def __init__(self, num_animal_classes=14):\n",
    "        super(AnimalClassifier, self).__init__()\n",
    "        self.model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # Pretrained model w/ weights\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_animal_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1716240422934,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "Vo2llioX3zej"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AnimalClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716240422935,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "Fu6-33V234gB"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Use label smoothing\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)  # Use SGD with weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716240422935,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "FZMVZUbL3-9H"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716240422935,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "Y1E_cqPx4HVG"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1716240423116,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "xKmiEXBA4d6c",
    "outputId": "a15ddd0a-5924-414f-bea5-8e25d1012bbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnimalClassifier(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_preds += torch.sum(preds == labels.data)\n",
    "            total_preds += len(labels)\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    accuracy = correct_preds.double() / total_preds\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2725414,
     "status": "ok",
     "timestamp": 1716243148528,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "kj1w_5jP4ir8",
    "outputId": "117d3d58-5926-46ab-a826-61944a183d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 2.3143, Val Loss: 1.2562, Val Accuracy: 0.7619\n",
      "Epoch 2/50, Train Loss: 0.9390, Val Loss: 1.0513, Val Accuracy: 0.8333\n",
      "Epoch 3/50, Train Loss: 0.7663, Val Loss: 1.1051, Val Accuracy: 0.8095\n",
      "Epoch 4/50, Train Loss: 0.7047, Val Loss: 1.0530, Val Accuracy: 0.8333\n",
      "Epoch 5/50, Train Loss: 0.6644, Val Loss: 1.0073, Val Accuracy: 0.8175\n",
      "Epoch 6/50, Train Loss: 0.6496, Val Loss: 1.0440, Val Accuracy: 0.8175\n",
      "Epoch 7/50, Train Loss: 0.6246, Val Loss: 0.9853, Val Accuracy: 0.8413\n",
      "Epoch 8/50, Train Loss: 0.6087, Val Loss: 0.9698, Val Accuracy: 0.8333\n",
      "Epoch 9/50, Train Loss: 0.6034, Val Loss: 0.9682, Val Accuracy: 0.8492\n",
      "Epoch 10/50, Train Loss: 0.5991, Val Loss: 0.9695, Val Accuracy: 0.8492\n",
      "Epoch 11/50, Train Loss: 0.5960, Val Loss: 0.9724, Val Accuracy: 0.8413\n",
      "Epoch 12/50, Train Loss: 0.5930, Val Loss: 0.9700, Val Accuracy: 0.8413\n",
      "Epoch 13/50, Train Loss: 0.5936, Val Loss: 0.9717, Val Accuracy: 0.8571\n",
      "Epoch 14/50, Train Loss: 0.5921, Val Loss: 0.9706, Val Accuracy: 0.8492\n",
      "Early stopping triggered after 14 epochs\n"
     ]
    }
   ],
   "source": [
    "# Received a os.fork() is incompatible with multithreaded code\n",
    "# Chat GPT suggested the following fix\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Set the start method to 'spawn' at the beginning of your script\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model with early stopping\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1716245589317,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "ZEUEP4zQLEH3"
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'animal_classifier_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1716243188977,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "kgjlMa7_EHw8"
   },
   "outputs": [],
   "source": [
    "# Function to classify an image and return the animal and predator class\n",
    "def classify_image(image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = val_transforms(image).unsqueeze(0).to(device)\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        animal_class = animal_classes[predicted.item()]\n",
    "        predator_class = predator_classes[predator_mapping[animal_class]]\n",
    "        return animal_class, predator_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "executionInfo": {
     "elapsed": 2197,
     "status": "ok",
     "timestamp": 1716243492349,
     "user": {
      "displayName": "Sandy Griffin",
      "userId": "01238413906686040842"
     },
     "user_tz": 240
    },
    "id": "jNgK4BcRE1MU",
    "outputId": "ce7eac59-f3c5-4663-87bb-2f3340382116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://ace4bff556d9a7f43c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ace4bff556d9a7f43c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "# Set up the Gradio interface\n",
    "def predict(image):\n",
    "    animal_class, predator_class = classify_image(image)\n",
    "    return f\"Animal: {animal_class}, Category: {predator_class}\"\n",
    "\n",
    "interface = gr.Interface(fn=predict, inputs=gr.Image(type=\"pil\"), outputs=gr.Text(), title=\"Animal Classifier\")\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8Kk37sH3A/HFEOF1Vys1e",
   "mount_file_id": "13ZIMBBWgE2vq-YjU5OR7z7OMpf_w3jbX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
